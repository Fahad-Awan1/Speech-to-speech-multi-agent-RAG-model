This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
app.py
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
venv
.env
</file>

<file path="app.py">
import os
import io
import time
import tempfile
from pathlib import Path

import streamlit as st
from audio_recorder_streamlit import audio_recorder
from pydub import AudioSegment
import requests
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from groq import Groq
from dotenv import load_dotenv
import docx2txt
from pypdf import PdfReader

# ---------------------------
# Load API Keys (env + secrets fallback)
# ---------------------------
load_dotenv()  # loads local .env if exists


def get_secret(key: str) -> str:
    """Try .env first, then Streamlit secrets"""
    return os.getenv(key) or st.secrets.get(key)


ASSEMBLYAI_API_KEY = get_secret("ASSEMBLYAI_API_KEY")
GROQ_API_KEY = get_secret("GROQ_API_KEY")

if not ASSEMBLYAI_API_KEY:
    raise RuntimeError("‚ùå ASSEMBLYAI_API_KEY not set (check .env or secrets.toml)")
if not GROQ_API_KEY:
    raise RuntimeError("‚ùå GROQ_API_KEY not set (check .env or secrets.toml)")

# ---------------------------
# TTS (choose one)
# ---------------------------
USE_COQUI = True
if USE_COQUI:
    from TTS.api import TTS
else:
    from gtts import gTTS


# ---------------------------
# Utilities
# ---------------------------
def load_text_from_filelike(filename: str, data: bytes) -> str:
    suffix = Path(filename).suffix.lower()
    if suffix == ".pdf":
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(data)
            tmp.flush()
            text = []
            reader = PdfReader(tmp.name)
            for page in reader.pages:
                text.append(page.extract_text() or "")
            return "\n".join(text)
    elif suffix == ".docx":
        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
            tmp.write(data)
            tmp.flush()
            return docx2txt.process(tmp.name) or ""
    elif suffix in (".txt", ".md"):
        return data.decode("utf-8", errors="ignore")
    else:
        return ""


def chunk_text(text: str, size=800, overlap=120):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + size, len(text))
        chunk = text[start:end]
        if chunk.strip():
            chunks.append(chunk.strip())
        start += size - overlap
    return chunks


# ---------------------------
# Index / Retrieval
# ---------------------------
class SimpleFAISS:
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        self.embedder = SentenceTransformer(model_name)
        self.texts = []
        self.sources = []
        self.index = None
        self.dim = self.embedder.get_sentence_embedding_dimension()

    def build(self, docs: list[tuple[str, str]]):
        self.texts = [t for t, _ in docs]
        self.sources = [s for _, s in docs]
        X = self.embedder.encode(
            self.texts, normalize_embeddings=True, convert_to_numpy=True
        )
        self.index = faiss.IndexFlatIP(self.dim)
        self.index.add(X)

    def search(self, query: str, k=5):
        q = self.embedder.encode(
            [query], normalize_embeddings=True, convert_to_numpy=True
        )
        D, I = self.index.search(q, k)
        results = []
        for idx, score in zip(I[0], D[0]):
            if 0 <= idx < len(self.texts):
                results.append(
                    {
                        "text": self.texts[idx],
                        "source": self.sources[idx],
                        "score": float(score),
                    }
                )
        return results


# ---------------------------
# ASR: AssemblyAI
# ---------------------------
class AssemblyAI_ASR:
    def __init__(self):
        self.api_key = ASSEMBLYAI_API_KEY
        self.base_url = "https://api.assemblyai.com/v2"
        self.headers = {"authorization": self.api_key}

    def upload(self, wav_bytes: bytes) -> str:
        response = requests.post(
            f"{self.base_url}/upload", headers=self.headers, data=wav_bytes
        )
        response.raise_for_status()
        return response.json()["upload_url"]

    def transcribe(self, wav_bytes: bytes) -> str:
        audio_url = self.upload(wav_bytes)
        data = {"audio_url": audio_url}
        response = requests.post(
            f"{self.base_url}/transcript", json=data, headers=self.headers
        )
        response.raise_for_status()
        transcript_id = response.json()["id"]
        polling_endpoint = f"{self.base_url}/transcript/{transcript_id}"

        while True:
            result = requests.get(polling_endpoint, headers=self.headers).json()
            if result["status"] == "completed":
                return result["text"]
            elif result["status"] == "error":
                raise RuntimeError(f"Transcription failed: {result['error']}")
            time.sleep(3)


# ---------------------------
# LLM (Groq) - RAG generator
# ---------------------------
class RAGGroq:
    def __init__(self, model="llama3-70b-8192", temperature=0.2):
        self.client = Groq(api_key=GROQ_API_KEY)
        self.model = model
        self.temperature = temperature

    def generate(self, query: str, contexts: list[dict]) -> str:
        context_block = "\n\n".join(
            [
                f"[{i + 1}] {c['text']}\n(Source: {c['source']})"
                for i, c in enumerate(contexts)
            ]
        )
        prompt = f"""You are an assistant answering strictly from the provided context.
If the answer isn't in the context, say you don't find it in the documents.

Question:
{query}

Context:
{context_block}

Answer (concise, factual):"""
        resp = self.client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            messages=[
                {"role": "system", "content": "You are a helpful RAG assistant."},
                {"role": "user", "content": prompt},
            ],
        )
        return resp.choices[0].message.content.strip()


# ---------------------------
# TTS
# ---------------------------
class TTSWrapper:
    def __init__(self):
        if USE_COQUI:
            self.tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")
        else:
            self.tts = None

    def synth(self, text: str) -> bytes:
        if USE_COQUI:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                self.tts.tts_to_file(text=text, file_path=tmp.name)
                return Path(tmp.name).read_bytes()
        else:
            t = gTTS(text=text, lang="en")
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
                t.save(tmp.name)
                mp3 = AudioSegment.from_file(tmp.name, format="mp3")
            wav_bytes = io.BytesIO()
            mp3.export(wav_bytes, format="wav")
            return wav_bytes.getvalue()


# ---------------------------
# Streamlit UI
# ---------------------------
st.set_page_config(
    page_title="Speech-to-Speech Doc Assistant", page_icon="üó£Ô∏è", layout="centered"
)
st.title("üó£Ô∏è Speech-to-Speech Document Assistant")
st.caption("Speak a question about your uploaded documents; get a spoken answer.")

with st.sidebar:
    st.header("üìÑ Upload & Index")
    files = st.file_uploader(
        "Upload PDF/DOCX/TXT",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True,
    )
    build_btn = st.button("Build / Rebuild Index")
    st.divider()
    st.header("‚öôÔ∏è Settings")
    top_k = st.slider("Retriever top_k", 1, 10, 5)
    llm_model = st.selectbox(
        "Groq LLM", ["llama3-70b-8192", "mixtral-8x7b-32768"], index=0
    )

if "retriever" not in st.session_state:
    st.session_state.retriever = None

if build_btn:
    if not files:
        st.warning("Upload at least one document.")
    else:
        docs = []
        for f in files:
            raw = f.read()
            text = load_text_from_filelike(f.name, raw)
            if not text.strip():
                continue
            chunks = chunk_text(text, size=800, overlap=120)
            for ch in chunks:
                docs.append((ch, f.name))
        if not docs:
            st.error("No text extracted. Are the PDFs scanned?")
        else:
            retr = SimpleFAISS()
            retr.build(docs)
            st.session_state.retriever = retr
            st.success(f"Indexed {len(retr.texts)} chunks from {len(files)} file(s).")

st.divider()
st.subheader("üé§ Ask by voice")

asr = AssemblyAI_ASR()
tts = TTSWrapper()
generator = None
if st.session_state.retriever:
    try:
        generator = RAGGroq(model=llm_model, temperature=0.2)
    except Exception as e:
        st.error(f"Groq init error: {e}")

audio_bytes = audio_recorder(
    text="Click to record / stop", pause_threshold=2.0, sample_rate=16000
)
if audio_bytes:
    st.audio(audio_bytes, format="audio/wav")
    with st.spinner("Transcribing (AssemblyAI)..."):
        query_text = asr.transcribe(audio_bytes)
    st.write(f"üóíÔ∏è *Heard:* `{query_text}`")

    if not query_text.strip():
        st.error("Didn't catch that. Try again.")
    elif not st.session_state.retriever:
        st.warning("Build the index first from the sidebar.")
    elif not generator:
        st.warning("LLM not ready. Check GROQ_API_KEY and try again.")
    else:
        with st.spinner("Retrieving..."):
            ctx = st.session_state.retriever.search(query_text, k=top_k)
        with st.expander("üîé Retrieved context"):
            for i, c in enumerate(ctx, 1):
                st.markdown(
                    f"**{i}. {c['source']}** (score={c['score']:.3f})\n\n{c['text'][:600]}{'...' if len(c['text']) > 600 else ''}"
                )

        with st.spinner("Generating answer..."):
            answer = generator.generate(query_text, ctx)

        with st.spinner("Synthesizing speech..."):
            out_wav = tts.synth(answer)

        st.markdown("### ‚ñ∂Ô∏è Answer (audio)")
        st.audio(out_wav, format="audio/wav")
        with st.expander("üìù (Dev view) Text answer"):
            st.write(answer)
</file>

<file path="requirements.txt">
streamlit
audio-recorder-streamlit
pydub
requests
faiss-cpu
numpy
sentence-transformers
groq
python-dotenv
docx2txt
pypdf
TTS
gTTS
</file>

</files>
